{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import string\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Embedding, Bidirectional\n",
    "from keras.layers.core import SpatialDropout1D\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from keras.datasets import imdb\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "from gensim import corpora\n",
    "\n",
    "# def parsePhrases(stopWords, engStemmer, phrases):\n",
    "#     print \"parse the phrases with stopwords and stemmer\"\n",
    "#     processedPhrases = []\n",
    "#     for phrase in phrases:\n",
    "#         tokens = word_tokenize(phrase)\n",
    "#         parsedWords = []\n",
    "#         for t in tokens:\n",
    "#             if t not in stopWords:\n",
    "#                 parsedWords.append(engStemmer.stem(t))\n",
    "#         processedPhrases.append(parsedWords)\n",
    "#     return processedPhrases\n",
    "postProcessedTrainPhrases = []\n",
    "postProcessedTestPhrases = []\n",
    "\n",
    "def preprocessData():\n",
    "    print(\"Loading and preprocessing data...\")\n",
    "    # load training and testing data\n",
    "    with open('labeled_document2.json') as json_data:\n",
    "        allTrainData = json.load(json_data)\n",
    "\n",
    "\n",
    "    trainPhrases, testPhrases, trainLabel,testLabel = train_test_split(allTrainData['Comment'], allTrainData['CommentLabel'], test_size=0.2, random_state=42)\n",
    "#     print(testPhrases[0:100])\n",
    "    punctuation = list(string.punctuation)\n",
    "    stopWords = stopwords.words('english') + punctuation \n",
    "\n",
    "    engStemmer = SnowballStemmer('english')\n",
    "    #postProcessedTrainPhrases = []\n",
    "#     for phrase in trainPhrases:\n",
    "#         uni_doc = unicode(phrase, errors='replace')\n",
    "#         tokens = word_tokenize(uni_doc)\n",
    "#         filtered = [word for word in tokens if word not in stop_words]\n",
    "#         try:\n",
    "#             stemmed = [stemmer.stem(word) for word in filtered]\n",
    "#         except UnicodeDecodeError:\n",
    "#             print(word)\n",
    "#         postProcessedTrainPhrases.append(parsedWords)\n",
    "\n",
    "#     for phrase in testPhrases:\n",
    "#         uni_doc = unicode(phrase, errors='replace')\n",
    "#         tokens = word_tokenize(uni_doc)\n",
    "#         filtered = [word for word in tokens if word not in stop_words]\n",
    "#         try:\n",
    "#             stemmed = [stemmer.stem(word) for word in filtered]\n",
    "#         except UnicodeDecodeError:\n",
    "#             print(word)\n",
    "#         postProcessedTestPhrases.append(parsedWords)\n",
    "    for phrase in trainPhrases:\n",
    "        if not isinstance(phrase, str):\n",
    "            continue\n",
    "        tokens = word_tokenize(phrase)\n",
    "        parsedWords = []\n",
    "        for t in tokens:\n",
    "            if t not in stopWords:\n",
    "                parsedWords.append(engStemmer.stem(t))\n",
    "        postProcessedTrainPhrases.append(parsedWords)\n",
    "\n",
    "    for phrase in testPhrases:\n",
    "        if not isinstance(phrase, str):\n",
    "            continue\n",
    "        tokens = word_tokenize(phrase)\n",
    "        parsedWords = []\n",
    "        for t in tokens:\n",
    "            if t not in stopWords:\n",
    "                parsedWords.append(engStemmer.stem(t))\n",
    "        postProcessedTestPhrases.append(parsedWords)\n",
    "    return (trainLabel,testLabel)\n",
    "\n",
    "\n",
    "def convertPhrasesToIDs(phrases):\n",
    "    print (\"converting the phrases to id to be processed\")\n",
    "    wordIDs = []\n",
    "    wordIDLens = []\n",
    "    for phrase in phrases:\n",
    "        ids = []\n",
    "        for word in phrase:\n",
    "            ids.append(toIDMap.token2id[word])\n",
    "        wordIDs.append(ids)\n",
    "        wordIDLens.append(len(ids))\n",
    "    return ( wordIDs, wordIDLens )\n",
    "\n",
    "def findSequenceLen(wordListLen):\n",
    "    print( \"calculate the norm sequence length\")\n",
    "    wordLenMean = np.mean(wordListLen)\n",
    "    wordLenStd = np.std(wordListLen)\n",
    "    return np.round(wordLenMean + 3 * wordLenStd).astype(int)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preprocessing data...\n"
     ]
    }
   ],
   "source": [
    "(trainSenti, testSenti) = preprocessData()\n",
    "# process training data and testing data\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converting the phrases to id to be processed\n",
      "converting the phrases to id to be processed\n",
      "calculate the norm sequence length\n",
      "pad sequence\n",
      "categorize the labels\n",
      "Epoch 1/3\n",
      "3394/3394 [==============================] - 11s 3ms/step - loss: 0.5299 - acc: 0.8079\n",
      "Epoch 2/3\n",
      "3394/3394 [==============================] - 9s 3ms/step - loss: 0.4493 - acc: 0.8385\n",
      "Epoch 3/3\n",
      "3394/3394 [==============================] - 8s 2ms/step - loss: 0.4183 - acc: 0.8385\n",
      "acc: 82.10%\n"
     ]
    }
   ],
   "source": [
    "toIDMap = corpora.Dictionary(np.concatenate((postProcessedTrainPhrases, postProcessedTestPhrases), axis=0))\n",
    "allPhraseSize = len(toIDMap.keys())\n",
    "\n",
    "(trainWordIDs, trainWordIDLens) = convertPhrasesToIDs(postProcessedTrainPhrases)\n",
    "(testWordIDs, testWordIDLens) = convertPhrasesToIDs(postProcessedTestPhrases)\n",
    "\n",
    "sequenceLen = findSequenceLen(trainWordIDLens + testWordIDLens)\n",
    "\n",
    "print( \"pad sequence\")\n",
    "trainingData = sequence.pad_sequences(np.array(trainWordIDs), maxlen=sequenceLen)\n",
    "testingData = sequence.pad_sequences(np.array(testWordIDs), maxlen=sequenceLen)\n",
    "\n",
    "print (\"categorize the labels\")\n",
    "#print len(np.unique(trainSenti))\n",
    "trainingDataLabel = np_utils.to_categorical(trainSenti, len(np.unique(trainSenti)))\n",
    "testingDataLabel = np_utils.to_categorical(testSenti, len(np.unique(testSenti)))\n",
    "# cvscores = []\n",
    "# batchSize = 15606\n",
    "# end = 156060\n",
    "# for i in range(0, 10):\n",
    "#     print(\"Fold:%d\" % i)\n",
    "#     tests = trainingData[i * batchSize:(i + 1) * batchSize]\n",
    "#     tests_label = trainingDataLabel[i * batchSize:(i + 1) * batchSize]\n",
    "#     if (i > 0):\n",
    "#         trains = np.concatenate((trainingData[0:(i * batchSize)], trainingData[(i + 1) * batchSize:end]), axis=0)\n",
    "#         trains_label = np.concatenate((trainingDataLabel[0:(i * batchSize)], trainingDataLabel[(i + 1) * batchSize:end]), axis=0)\n",
    "#     else:\n",
    "#         trains = trainingData[(i + 1) * batchSize:end]\n",
    "#         trains_label = trainingDataLabel[(i + 1) * batchSize:end]\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(allPhraseSize, 128))\n",
    "model.add(SpatialDropout1D(0.1))\n",
    "model.add(Bidirectional(LSTM(128)))\n",
    "#model.add(Bidirectional(LSTM(128)))\n",
    "#model.add(Flatten())\n",
    "model.add(Dense(len(np.unique(trainSenti))))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# model = Sequential()\n",
    "# model.add(Embedding(allPhraseSize, 128, dropout=0.2))\n",
    "# model.add(LSTM(128, dropout_W=0.2, dropout_U=0.2))\n",
    "# model.add(Dense(num_labels))\n",
    "# model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(trainingData,trainingDataLabel , epochs=3, batch_size=256, verbose=1)\n",
    "# evaluate the model\n",
    "scores = model.evaluate(testingData, testingDataLabel, verbose=0)\n",
    "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1] * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
