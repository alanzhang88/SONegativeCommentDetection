{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import string\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Embedding, Bidirectional\n",
    "from keras.layers.core import SpatialDropout1D\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from keras.datasets import imdb\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.models import model_from_json\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "from gensim import corpora\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# def parsePhrases(stopWords, engStemmer, phrases):\n",
    "#     print \"parse the phrases with stopwords and stemmer\"\n",
    "#     processedPhrases = []\n",
    "#     for phrase in phrases:\n",
    "#         tokens = word_tokenize(phrase)\n",
    "#         parsedWords = []\n",
    "#         for t in tokens:\n",
    "#             if t not in stopWords:\n",
    "#                 parsedWords.append(engStemmer.stem(t))\n",
    "#         processedPhrases.append(parsedWords)\n",
    "#     return processedPhrases\n",
    "postProcessedTrainPhrases = []\n",
    "postProcessedTestPhrases = []\n",
    "\n",
    "def preprocessData():\n",
    "    print(\"Loading and preprocessing data...\")\n",
    "    # load training and testing data\n",
    "    with open('labeled_document2.json') as json_data:\n",
    "        allTrainData = json.load(json_data)\n",
    "\n",
    "\n",
    "    trainPhrases, testPhrases, trainLabel,testLabel = train_test_split(allTrainData['Comment'], allTrainData['CommentLabel'], test_size=0.2, random_state=42)\n",
    "    \n",
    "#     print(testPhrases[0:100])\n",
    "    punctuation = list(string.punctuation)\n",
    "    stopWords = stopwords.words('english') + punctuation \n",
    "\n",
    "    engStemmer = SnowballStemmer('english')\n",
    "    #postProcessedTrainPhrases = []\n",
    "#     for phrase in trainPhrases:\n",
    "#         uni_doc = unicode(phrase, errors='replace')\n",
    "#         tokens = word_tokenize(uni_doc)\n",
    "#         filtered = [word for word in tokens if word not in stop_words]\n",
    "#         try:\n",
    "#             stemmed = [stemmer.stem(word) for word in filtered]\n",
    "#         except UnicodeDecodeError:\n",
    "#             print(word)\n",
    "#         postProcessedTrainPhrases.append(parsedWords)\n",
    "\n",
    "#     for phrase in testPhrases:\n",
    "#         uni_doc = unicode(phrase, errors='replace')\n",
    "#         tokens = word_tokenize(uni_doc)\n",
    "#         filtered = [word for word in tokens if word not in stop_words]\n",
    "#         try:\n",
    "#             stemmed = [stemmer.stem(word) for word in filtered]\n",
    "#         except UnicodeDecodeError:\n",
    "#             print(word)\n",
    "#         postProcessedTestPhrases.append(parsedWords)\n",
    "    for phrase in trainPhrases:\n",
    "        if not isinstance(phrase, str):\n",
    "            continue\n",
    "        tokens = word_tokenize(phrase)\n",
    "        parsedWords = []\n",
    "        for t in tokens:\n",
    "            if t not in stopWords:\n",
    "                parsedWords.append(engStemmer.stem(t))\n",
    "        postProcessedTrainPhrases.append(parsedWords)\n",
    "\n",
    "    for phrase in testPhrases:\n",
    "        if not isinstance(phrase, str):\n",
    "            continue\n",
    "        tokens = word_tokenize(phrase)\n",
    "        parsedWords = []\n",
    "        for t in tokens:\n",
    "            if t not in stopWords:\n",
    "                parsedWords.append(engStemmer.stem(t))\n",
    "        postProcessedTestPhrases.append(parsedWords)\n",
    "    return (trainLabel,testLabel)\n",
    "\n",
    "\n",
    "def convertPhrasesToIDs(phrases):\n",
    "    print (\"converting the phrases to id to be processed\")\n",
    "    wordIDs = []\n",
    "    wordIDLens = []\n",
    "    for phrase in phrases:\n",
    "        ids = []\n",
    "        for word in phrase:\n",
    "            ids.append(toIDMap.token2id[word])\n",
    "        wordIDs.append(ids)\n",
    "        wordIDLens.append(len(ids))\n",
    "    return ( wordIDs, wordIDLens )\n",
    "\n",
    "def findSequenceLen(wordListLen):\n",
    "    print( \"calculate the norm sequence length\")\n",
    "    wordLenMean = np.mean(wordListLen)\n",
    "    wordLenStd = np.std(wordListLen)\n",
    "    return np.round(wordLenMean + 3 * wordLenStd).astype(int)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converting the phrases to id to be processed\n",
      "converting the phrases to id to be processed\n",
      "calculate the norm sequence length\n",
      "pad sequence\n",
      "categorize the labels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dima/.pyenv/versions/3.5.2/lib/python3.5/site-packages/sklearn/utils/deprecation.py:77: DeprecationWarning: Function _ratio_float is deprecated; Use a float for 'ratio' is deprecated from version 0.2. The support will be removed in 0.4. Use a dict, str, or a callable instead.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "5692/5692 [==============================] - 20s 3ms/step - loss: 0.6677 - acc: 0.6196\n",
      "Epoch 2/3\n",
      "5692/5692 [==============================] - 17s 3ms/step - loss: 0.4168 - acc: 0.8349\n",
      "Epoch 3/3\n",
      "5692/5692 [==============================] - 16s 3ms/step - loss: 0.2733 - acc: 0.8927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dima/.pyenv/versions/3.5.2/lib/python3.5/site-packages/sklearn/utils/deprecation.py:77: DeprecationWarning: Function _ratio_float is deprecated; Use a float for 'ratio' is deprecated from version 0.2. The support will be removed in 0.4. Use a dict, str, or a callable instead.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 85.87%\n"
     ]
    }
   ],
   "source": [
    "(trainSenti, testSenti) = preprocessData()\n",
    "# process training data and testing data\n",
    "\n",
    "toIDMap = corpora.Dictionary(np.concatenate((postProcessedTrainPhrases, postProcessedTestPhrases), axis=0))\n",
    "allPhraseSize = len(toIDMap.keys())\n",
    "\n",
    "(trainWordIDs, trainWordIDLens) = convertPhrasesToIDs(postProcessedTrainPhrases)\n",
    "(testWordIDs, testWordIDLens) = convertPhrasesToIDs(postProcessedTestPhrases)\n",
    "\n",
    "sequenceLen = findSequenceLen(trainWordIDLens + testWordIDLens)\n",
    "\n",
    "print( \"pad sequence\")\n",
    "trainingData = sequence.pad_sequences(np.array(trainWordIDs), maxlen=sequenceLen)\n",
    "testingData = sequence.pad_sequences(np.array(testWordIDs), maxlen=sequenceLen)\n",
    "\n",
    "sm = SMOTE(random_state=12, ratio = 1.0)\n",
    "trainingData, trainSenti = sm.fit_sample(trainingData, trainSenti)\n",
    "\n",
    "print (\"categorize the labels\")\n",
    "#print len(np.unique(trainSenti))\n",
    "trainingDataLabel = np_utils.to_categorical(trainSenti, len(np.unique(trainSenti)))\n",
    "\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(allPhraseSize, 128))\n",
    "model.add(SpatialDropout1D(0.1))\n",
    "model.add(Bidirectional(LSTM(128)))\n",
    "#model.add(Bidirectional(LSTM(128)))\n",
    "#model.add(Flatten())\n",
    "model.add(Dense(len(np.unique(trainSenti))))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# model = Sequential()\n",
    "# model.add(Embedding(allPhraseSize, 128, dropout=0.2))\n",
    "# model.add(LSTM(128, dropout_W=0.2, dropout_U=0.2))\n",
    "# model.add(Dense(num_labels))\n",
    "# model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(trainingData,trainingDataLabel , epochs=3, batch_size=256, verbose=1)\n",
    "# evaluate the model\n",
    "testingData, testSenti = sm.fit_sample(testingData, testSenti)\n",
    "testingDataLabel = np_utils.to_categorical(testSenti, len(np.unique(testSenti)))\n",
    "scores = model.evaluate(testingData, testingDataLabel, verbose=0)\n",
    "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1] * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.48522060540876627, 0.8068315667594982]\n"
     ]
    }
   ],
   "source": [
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictedRes = model.predict_proba(testingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "model_json = model.to_json()\n",
    "with open(\"LSTM.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"LSTM.h5\")\n",
    "print(\"Saved model to disk\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n"
     ]
    }
   ],
   "source": [
    "json_file = open('LSTM.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"LSTM.h5\")\n",
    "print(\"Loaded model from disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_res = loaded_model.predict_proba(testingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
