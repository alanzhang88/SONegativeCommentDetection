{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import string\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Embedding, Bidirectional, GlobalMaxPool1D\n",
    "from keras.layers.core import SpatialDropout1D\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from keras.datasets import imdb\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.models import model_from_json\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "from gensim import corpora\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# def parsePhrases(stopWords, engStemmer, phrases):\n",
    "#     print \"parse the phrases with stopwords and stemmer\"\n",
    "#     processedPhrases = []\n",
    "#     for phrase in phrases:\n",
    "#         tokens = word_tokenize(phrase)\n",
    "#         parsedWords = []\n",
    "#         for t in tokens:\n",
    "#             if t not in stopWords:\n",
    "#                 parsedWords.append(engStemmer.stem(t))\n",
    "#         processedPhrases.append(parsedWords)\n",
    "#     return processedPhrases\n",
    "postProcessedTrainPhrases = []\n",
    "postProcessedTestPhrases = []\n",
    "\n",
    "def preprocessData():\n",
    "    print(\"Loading and preprocessing data...\")\n",
    "    # load training and testing data\n",
    "    with open('labeled_document2.json') as json_data:\n",
    "        allTrainData = json.load(json_data)\n",
    "\n",
    "\n",
    "    trainPhrases, testPhrases, trainLabel,testLabel = train_test_split(allTrainData['Comment'], allTrainData['CommentLabel'], test_size=0.2, random_state=42)\n",
    "    \n",
    "#     print(testPhrases[0:100])\n",
    "    punctuation = list(string.punctuation)\n",
    "    stopWords = stopwords.words('english') + punctuation \n",
    "\n",
    "    engStemmer = SnowballStemmer('english')\n",
    "    for phrase in trainPhrases:\n",
    "        if not isinstance(phrase, str):\n",
    "            continue\n",
    "        tokens = word_tokenize(phrase)\n",
    "        parsedWords = []\n",
    "        for t in tokens:\n",
    "            if t not in stopWords:\n",
    "                parsedWords.append(engStemmer.stem(t))\n",
    "        postProcessedTrainPhrases.append(parsedWords)\n",
    "\n",
    "    for phrase in testPhrases:\n",
    "        if not isinstance(phrase, str):\n",
    "            continue\n",
    "        tokens = word_tokenize(phrase)\n",
    "        parsedWords = []\n",
    "        for t in tokens:\n",
    "            if t not in stopWords:\n",
    "                parsedWords.append(engStemmer.stem(t))\n",
    "        postProcessedTestPhrases.append(parsedWords)\n",
    "    return (trainLabel,testLabel)\n",
    "\n",
    "\n",
    "def convertPhrasesToIDs(phrases):\n",
    "    print (\"converting the phrases to id to be processed\")\n",
    "    wordIDs = []\n",
    "    wordIDLens = []\n",
    "    for phrase in phrases:\n",
    "        ids = []\n",
    "        for word in phrase:\n",
    "            ids.append(toIDMap.token2id[word])\n",
    "        wordIDs.append(ids)\n",
    "        wordIDLens.append(len(ids))\n",
    "    return ( wordIDs, wordIDLens )\n",
    "\n",
    "def findSequenceLen(wordListLen):\n",
    "    print( \"calculate the norm sequence length\")\n",
    "    wordLenMean = np.mean(wordListLen)\n",
    "    wordLenStd = np.std(wordListLen)\n",
    "    return np.round(wordLenMean + 3 * wordLenStd).astype(int)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preprocessing data...\n",
      "3394 3394\n",
      "converting the phrases to id to be processed\n",
      "converting the phrases to id to be processed\n",
      "calculate the norm sequence length\n",
      "pad sequence\n",
      "(3394, 46)\n",
      "categorize the labels\n",
      "(3394, 2)\n",
      "Epoch 1/10\n",
      "3394/3394 [==============================] - 31s 9ms/step - loss: 0.5147 - acc: 0.7955\n",
      "Epoch 2/10\n",
      "3394/3394 [==============================] - 17s 5ms/step - loss: 0.4355 - acc: 0.8385\n",
      "Epoch 3/10\n",
      "3394/3394 [==============================] - 18s 5ms/step - loss: 0.3959 - acc: 0.8385\n",
      "Epoch 4/10\n",
      "3394/3394 [==============================] - 13s 4ms/step - loss: 0.2905 - acc: 0.8721\n",
      "Epoch 5/10\n",
      "3394/3394 [==============================] - 12s 4ms/step - loss: 0.2177 - acc: 0.9107\n",
      "Epoch 6/10\n",
      "3394/3394 [==============================] - 13s 4ms/step - loss: 0.1608 - acc: 0.9369\n",
      "Epoch 7/10\n",
      "3394/3394 [==============================] - 12s 4ms/step - loss: 0.1213 - acc: 0.9567\n",
      "Epoch 8/10\n",
      "3394/3394 [==============================] - 11s 3ms/step - loss: 0.0902 - acc: 0.9643\n",
      "Epoch 9/10\n",
      "3394/3394 [==============================] - 10s 3ms/step - loss: 0.0733 - acc: 0.9744\n",
      "Epoch 10/10\n",
      "3394/3394 [==============================] - 11s 3ms/step - loss: 0.0567 - acc: 0.9803\n",
      "acc: 79.62%\n"
     ]
    }
   ],
   "source": [
    "(trainSenti, testSenti) = preprocessData()\n",
    "# process training data and testing data\n",
    "\n",
    "print(len(postProcessedTrainPhrases), len(trainSenti))\n",
    "toIDMap = corpora.Dictionary(np.concatenate((postProcessedTrainPhrases, postProcessedTestPhrases), axis=0))\n",
    "allPhraseSize = len(toIDMap.keys())\n",
    "\n",
    "(trainWordIDs, trainWordIDLens) = convertPhrasesToIDs(postProcessedTrainPhrases)\n",
    "(testWordIDs, testWordIDLens) = convertPhrasesToIDs(postProcessedTestPhrases)\n",
    "\n",
    "sequenceLen = findSequenceLen(trainWordIDLens + testWordIDLens)\n",
    "\n",
    "print( \"pad sequence\")\n",
    "trainingData = sequence.pad_sequences(np.array(trainWordIDs), maxlen=sequenceLen)\n",
    "testingData = sequence.pad_sequences(np.array(testWordIDs), maxlen=sequenceLen)\n",
    "print(trainingData.shape)\n",
    "\n",
    "print (\"categorize the labels\")\n",
    "#print len(np.unique(trainSenti))\n",
    "trainingDataLabel = np_utils.to_categorical(trainSenti, len(np.unique(trainSenti)))\n",
    "\n",
    "print(trainingDataLabel.shape)\n",
    "\n",
    "# model = Sequential()\n",
    "# model.add(Embedding(allPhraseSize, 128))\n",
    "# model.add(SpatialDropout1D(0.1))\n",
    "# model.add(Bidirectional(LSTM(128)))\n",
    "# #model.add(Bidirectional(LSTM(128)))\n",
    "# #model.add(Flatten())\n",
    "# model.add(Dense(len(np.unique(trainSenti))))\n",
    "# model.add(Activation('softmax'))\n",
    "# model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# model = Sequential()\n",
    "# model.add(Embedding(allPhraseSize, 128, dropout=0.2))\n",
    "# model.add(LSTM(128, dropout_W=0.2, dropout_U=0.2))\n",
    "# model.add(Dense(num_labels))\n",
    "# model.add(Activation('softmax'))\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(allPhraseSize, 128))\n",
    "model.add(Bidirectional(LSTM(128, return_sequences=True)))\n",
    "model.add(GlobalMaxPool1D())\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(128, activation=\"relu\"))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(len(np.unique(trainSenti))))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "model.fit(trainingData,trainingDataLabel , epochs=10, batch_size=256, verbose=1)\n",
    "# evaluate the model\n",
    "testingDataLabel = np_utils.to_categorical(testSenti, len(np.unique(testSenti)))\n",
    "scores = model.evaluate(testingData, testingDataLabel, verbose=0)\n",
    "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1] * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3394, 46) (3394, 2)\n"
     ]
    }
   ],
   "source": [
    "print(trainingData.shape, trainingDataLabel.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.48522060540876627, 0.8068315667594982]\n"
     ]
    }
   ],
   "source": [
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictedRes = model.predict_proba(testingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "model_json = model.to_json()\n",
    "with open(\"LSTM.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"LSTM.h5\")\n",
    "print(\"Saved model to disk\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n"
     ]
    }
   ],
   "source": [
    "json_file = open('LSTM.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"LSTM.h5\")\n",
    "print(\"Loaded model from disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_res = loaded_model.predict_proba(testingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.33525658 0.6647435 ]\n",
      " [0.06602536 0.9339746 ]\n",
      " [0.00454879 0.9954513 ]\n",
      " ...\n",
      " [0.8005882  0.19941178]\n",
      " [0.9947694  0.00523055]\n",
      " [0.693177   0.30682302]]\n"
     ]
    }
   ],
   "source": [
    "print(predict_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
