{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import string\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Embedding, Bidirectional, GlobalMaxPool1D\n",
    "from keras.layers.core import SpatialDropout1D\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from keras.datasets import imdb\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.models import model_from_json\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "from gensim import corpora\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "# def parsePhrases(stopWords, engStemmer, phrases):\n",
    "#     print \"parse the phrases with stopwords and stemmer\"\n",
    "#     processedPhrases = []\n",
    "#     for phrase in phrases:\n",
    "#         tokens = word_tokenize(phrase)\n",
    "#         parsedWords = []\n",
    "#         for t in tokens:\n",
    "#             if t not in stopWords:\n",
    "#                 parsedWords.append(engStemmer.stem(t))\n",
    "#         processedPhrases.append(parsedWords)\n",
    "#     return processedPhrases\n",
    "postProcessedTrainPhrases = []\n",
    "postProcessedTestPhrases = []\n",
    "\n",
    "def preprocessData():\n",
    "    print(\"Loading and preprocessing data...\")\n",
    "    # load training and testing data\n",
    "    with open('labeled_document_firstiter.json') as json_data:\n",
    "        allTrainData = json.load(json_data)\n",
    "    \n",
    "    with open('labeled_document_seconditer.json') as json_data:\n",
    "        allTrainData2 = json.load(json_data)\n",
    "\n",
    "    \n",
    "    trainPhrases, testPhrases, trainLabel,testLabel = train_test_split(allTrainData['Comment'] + allTrainData2['Comment'], allTrainData['CommentLabel']+allTrainData2['CommentLabel'], test_size=0.2, random_state=42)\n",
    "    \n",
    "#     print(testPhrases[0:100])\n",
    "    punctuation = list(string.punctuation)\n",
    "    stopWords = stopwords.words('english') + punctuation \n",
    "\n",
    "    engStemmer = SnowballStemmer('english')\n",
    "    for phrase in trainPhrases:\n",
    "        if not isinstance(phrase, str):\n",
    "            continue\n",
    "        tokens = word_tokenize(phrase)\n",
    "        parsedWords = []\n",
    "        for t in tokens:\n",
    "            if t not in stopWords:\n",
    "                parsedWords.append(engStemmer.stem(t))\n",
    "        postProcessedTrainPhrases.append(parsedWords)\n",
    "\n",
    "    for phrase in testPhrases:\n",
    "        if not isinstance(phrase, str):\n",
    "            continue\n",
    "        tokens = word_tokenize(phrase)\n",
    "        parsedWords = []\n",
    "        for t in tokens:\n",
    "            if t not in stopWords:\n",
    "                parsedWords.append(engStemmer.stem(t))\n",
    "        postProcessedTestPhrases.append(parsedWords)\n",
    "    return (trainLabel,testLabel)\n",
    "\n",
    "\n",
    "def convertPhrasesToIDs(phrases):\n",
    "    print (\"converting the phrases to id to be processed\")\n",
    "    wordIDs = []\n",
    "    wordIDLens = []\n",
    "    for phrase in phrases:\n",
    "        ids = []\n",
    "        for word in phrase:\n",
    "            ids.append(toIDMap.token2id[word])\n",
    "        wordIDs.append(ids)\n",
    "        wordIDLens.append(len(ids))\n",
    "    return ( wordIDs, wordIDLens )\n",
    "\n",
    "def findSequenceLen(wordListLen):\n",
    "    print( \"calculate the norm sequence length\")\n",
    "    wordLenMean = np.mean(wordListLen)\n",
    "    wordLenStd = np.std(wordListLen)\n",
    "    return np.round(wordLenMean + 3 * wordLenStd).astype(int)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preprocessing data...\n",
      "converting the phrases to id to be processed\n",
      "converting the phrases to id to be processed\n",
      "calculate the norm sequence length\n",
      "pad sequence\n",
      "(8980, 45)\n",
      "categorize the labels\n"
     ]
    }
   ],
   "source": [
    "(trainSenti, testSenti) = preprocessData()\n",
    "\n",
    "# process training data and testing data\n",
    "\n",
    "# print(len(postProcessedTrainPhrases), len(trainSenti))\n",
    "toIDMap = corpora.Dictionary(np.concatenate((postProcessedTrainPhrases, postProcessedTestPhrases), axis=0))\n",
    "allPhraseSize = len(toIDMap.keys())\n",
    "\n",
    "(trainWordIDs, trainWordIDLens) = convertPhrasesToIDs(postProcessedTrainPhrases)\n",
    "(testWordIDs, testWordIDLens) = convertPhrasesToIDs(postProcessedTestPhrases)\n",
    "\n",
    "sequenceLen = findSequenceLen(trainWordIDLens + testWordIDLens)\n",
    "\n",
    "print( \"pad sequence\")\n",
    "trainingData = sequence.pad_sequences(np.array(trainWordIDs), maxlen=sequenceLen)\n",
    "testingData = sequence.pad_sequences(np.array(testWordIDs), maxlen=sequenceLen)\n",
    "print(trainingData.shape)\n",
    "\n",
    "print (\"categorize the labels\")\n",
    "#print len(np.unique(trainSenti))\n",
    "trainingDataLabel = np_utils.to_categorical(trainSenti, len(np.unique(trainSenti)))\n",
    "\n",
    "# print(trainingDataLabel.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "4352/8980 [=============>................] - ETA: 33s - loss: 0.5190 - acc: 0.8352"
     ]
    }
   ],
   "source": [
    "\n",
    "# epochs = [5, 10, 50, 100, 500]\n",
    "# optimizer = ['sgd', 'RMSprop', 'adam']\n",
    "# activation = ['tanh','softmax','relu','sigmoid']\n",
    "# hid_size = [64, 128, 256]\n",
    "# dropoutrate = [0.0, 0.05, 0.1, 0.25, 0.5]\n",
    "embedding_size = 128\n",
    "# parameters = {'optimizer':('sgd', 'RMSprop', 'adam'), 'activation':[1, 10]}\n",
    "activation =  ['relu', 'tanh', 'sigmoid', 'hard_sigmoid', 'linear','softmax'] # softmax, softplus, softsign \n",
    "hidden_size = [64, 128, 256]\n",
    "# momentum = [0.0, 0.2, 0.4, 0.6, 0.8, 0.9]\n",
    "# learn_rate = [0.001, 0.01, 0.1, 0.2]\n",
    "dropout_rate = [0.0, 0.05, 0.1, 0.25, 0.5]\n",
    "# weight_constraint=[1, 2, 3, 4, 5]\n",
    "# neurons = [1, 5, 10, 15, 20, 25, 30]\n",
    "init = ['uniform', 'lecun_uniform', 'normal', 'zero', 'glorot_normal', 'glorot_uniform', 'he_normal', 'he_uniform']\n",
    "optimizer = [ 'SGD', 'RMSprop', 'Adam', 'Adamax', 'Nadam']\n",
    "epochs = [5, 10, 100] \n",
    "batch_size = [10, 100, 1000]\n",
    "\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(allPhraseSize, embedding_size))\n",
    "model.add(SpatialDropout1D(0.1))\n",
    "model.add(Bidirectional(LSTM(128, return_sequences=True)))\n",
    "model.add(Bidirectional(LSTM(128)))\n",
    "#model.add(Bidirectional(LSTM(128)))\n",
    "#model.add(Flatten())\n",
    "model.add(Dense(len(np.unique(trainSenti))))\n",
    "model.add(Activation('sigmoid'))\n",
    "# model.add(CRF(2, sparse_target=True))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "model.fit(trainingData,trainingDataLabel , epochs=2, batch_size=256, verbose=1)\n",
    "# # evaluate the model\n",
    "testingDataLabel = np_utils.to_categorical(testSenti, len(np.unique(testSenti)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "res = model.predict(testingData)\n",
    "res = [(np.array(l)/sum(l)).tolist() for l in res]\n",
    "# print(predicted)\n",
    "predicted = []\n",
    "negcount = 0\n",
    "poscount = 0\n",
    "for i in res:\n",
    "    if i[0] > i[1]:\n",
    "        negcount +=1\n",
    "        predicted.append(0)\n",
    "    else:\n",
    "        poscount +=1\n",
    "        predicted.append(1)\n",
    "\n",
    "print(\"negative: \", negcount)\n",
    "print(\"positive: \", poscount)\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(testSenti, predicted).ravel()\n",
    "print(tn, fp, fn, tp)\n",
    "report = precision_recall_fscore_support(testSenti, predicted)\n",
    "print(\"precision: \", report[0][0])\n",
    "print(\"recall: \", report[1][0])\n",
    "print(\"fbeta_score: \",report[2][0] )\n",
    "# print(report.fbeta_score)\n",
    "scores = model.evaluate(testingData, testingDataLabel, verbose=0)\n",
    "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1] * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "model_json = model.to_json()\n",
    "with open(\"LSTM.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"LSTM.h5\")\n",
    "print(\"Saved model to disk\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n"
     ]
    }
   ],
   "source": [
    "json_file = open('LSTM.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"LSTM.h5\")\n",
    "print(\"Loaded model from disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "432\n"
     ]
    }
   ],
   "source": [
    "print(3*3*3*4*2*2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Number of hidden layers\n",
    "2. Number of hidden units per layer (usually same number in each layer)\n",
    "3. Learning rate of the optimizer\n",
    "4. Dropout rate (in RNNs dropout is perhaps better applied to feed forward connections only)\n",
    "5. Number of iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 lstm\n",
    "negative:  310\n",
    "positive:  1936\n",
    "[[ 186  159]\n",
    " [ 124 1777]]\n",
    "precision:  0.6\n",
    "recall:  0.5391304347826087\n",
    "fbeta_score:  0.5679389312977099\n",
    "acc: 87.40%\n",
    "\n",
    "negative:  382\n",
    "positive:  1864\n",
    "[[ 211  134]\n",
    " [ 171 1730]]\n",
    "precision:  0.5523560209424084\n",
    "recall:  0.6115942028985507\n",
    "fbeta_score:  0.5804676753782669\n",
    "acc: 86.42%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://colah.github.io/posts/2015-08-Understanding-LSTMs/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
